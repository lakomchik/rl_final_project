{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agar.Env import AgarEnv\n",
    "import numpy as np\n",
    "from agar.Config import Config\n",
    "import time\n",
    "\n",
    "render = True\n",
    "num_agents = 1\n",
    "\n",
    "class Args:\n",
    "    \"\"\"\n",
    "    Class to hold the arguments for the environment.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.num_controlled_agent = num_agents\n",
    "        self.num_processes = 64\n",
    "        self.action_repeat = 1\n",
    "        self.total_step = 1e8\n",
    "        self.r_alpha = 0.1\n",
    "        self.r_beta = 0.1\n",
    "        self.seed = 42\n",
    "        self.gamma = 0.99\n",
    "        self.eval = True\n",
    "\n",
    "\n",
    "class ContEnvWrapper:\n",
    "    \"\"\"\n",
    "    Wrapper class for the continuous environment.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.env = AgarEnv(Args())\n",
    "        self.action_limits = np.array([[-1,1],[-1,1]])\n",
    "    \n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        Reset the environment and return the initial observation.\n",
    "        \"\"\"\n",
    "        obs = self.env.reset()\n",
    "        return obs\n",
    "    \n",
    "    def render(self):\n",
    "        \"\"\"\n",
    "        Render the environment.\n",
    "        \"\"\"\n",
    "        self.env.render(0,render_player=True)\n",
    "    \n",
    "    def close(self):\n",
    "        \"\"\"\n",
    "        Close the environment.\n",
    "        \"\"\"\n",
    "        self.env.close()\n",
    "    \n",
    "    def step(self, actions):\n",
    "        \"\"\"\n",
    "        Take a step in the environment given the actions and return the new observation, reward, and done flag.\n",
    "        \"\"\"\n",
    "        actions = np.array(actions).reshape(-1)\n",
    "        actions[2] = 1 if actions[2] > 0 else 0\n",
    "        obs, rewards, dones, infos, new_obs = self.env.step(actions)\n",
    "        return obs['t0'], rewards[0], dones[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize the policy like TransformerGaussianPolicy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class TransformerGaussianPolicy(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, nhead, num_encoder_layers):\n",
    "        super(TransformerGaussianPolicy, self).__init__()\n",
    "        \n",
    "        # Embedding for the input, increase the dimension for transformer\n",
    "        self.embedding = nn.Linear(input_dim, 512)\n",
    "        \n",
    "        # Transformer encoder\n",
    "        self.transformer = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(d_model=512, nhead=nhead),\n",
    "            num_layers=num_encoder_layers\n",
    "        )\n",
    "        \n",
    "        # Heads for mean and standard deviation\n",
    "        self.mu_head = nn.Linear(512, output_dim)\n",
    "        self.sigma_head = nn.Linear(512, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass of the TransformerGaussianPolicy network.\n",
    "        \n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor of shape (batch_size, input_dim).\n",
    "        \n",
    "        Returns:\n",
    "            mu (torch.Tensor): Mean tensor of shape (batch_size, output_dim).\n",
    "            sigma (torch.Tensor): Standard deviation tensor of shape (batch_size, output_dim).\n",
    "        \"\"\"\n",
    "        x = self.embedding(x)\n",
    "        \n",
    "        # Note: Transformer expects input in the format (sequence length, batch size, features)\n",
    "        # Here, we treat our input as a sequence of length 1.\n",
    "        x = x.unsqueeze(0)\n",
    "        x = self.transformer(x)\n",
    "        x = x.squeeze(0)\n",
    "        \n",
    "        mu = torch.tanh(self.mu_head(x))\n",
    "        sigma = F.softplus(self.sigma_head(x)) + 1e-5\n",
    "        return mu, sigma\n",
    "\n",
    "\n",
    "class GaussianBoostedPolicyNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(GaussianPolicyNetwork, self).__init__()\n",
    "        self.fc = nn.Linear(input_dim, 128)\n",
    "        self.mu_head = nn.Linear(128, output_dim)\n",
    "        self.sigma_head = nn.Linear(128, output_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass of the GaussianBoostedPolicyNetwork network.\n",
    "        \n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor of shape (batch_size, input_dim).\n",
    "        \n",
    "        Returns:\n",
    "            mu (torch.Tensor): Mean tensor of shape (batch_size, output_dim).\n",
    "            sigma (torch.Tensor): Standard deviation tensor of shape (batch_size, output_dim).\n",
    "        \"\"\"\n",
    "        x = torch.relu(self.fc(x))\n",
    "        mu = torch.tanh(self.mu_head(x))  # Mean\n",
    "        sigma = F.softplus(self.sigma_head(x)) + 1e-5  # Standard deviation\n",
    "        return mu, sigma\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "class GaussianPolicyNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    Gaussian Policy Network class.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        \"\"\"\n",
    "        Initialize the Gaussian Policy Network.\n",
    "\n",
    "        Args:\n",
    "            input_dim (int): Dimensionality of the input.\n",
    "            output_dim (int): Dimensionality of the output.\n",
    "        \"\"\"\n",
    "        super(GaussianPolicyNetwork, self).__init__()\n",
    "        self.fc = nn.Linear(input_dim, 128)\n",
    "        self.mu_head = nn.Linear(128, output_dim)\n",
    "        self.sigma_head = nn.Linear(128, output_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass of the network.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Mean and standard deviation tensors.\n",
    "        \"\"\"\n",
    "        x = torch.relu(self.fc(x))\n",
    "        mu = torch.tanh(self.mu_head(x))  # Mean\n",
    "        sigma = F.softplus(self.sigma_head(x)) + 1e-5  # Standard deviation\n",
    "        return mu, sigma\n",
    "\n",
    "def select_action(policy, state):\n",
    "    \"\"\"\n",
    "    Select an action based on the given policy and state.\n",
    "\n",
    "    Args:\n",
    "        policy (GaussianPolicyNetwork): Policy network.\n",
    "        state (np.array): Input state.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Action tensor.\n",
    "        torch.Tensor: Log probability tensor.\n",
    "    \"\"\"\n",
    "    state_tensor = torch.tensor(state, dtype=torch.float32)\n",
    "    mu, sigma = policy(state_tensor)\n",
    "    dist = torch.distributions.Normal(mu, sigma)\n",
    "    action = dist.sample()\n",
    "    log_prob = dist.log_prob(action).sum(-1)\n",
    "    return action, log_prob\n",
    "\n",
    "def train_policy(policy, optimizer, device, episodes=1000):\n",
    "    \"\"\"\n",
    "    Train the policy network.\n",
    "\n",
    "    Args:\n",
    "        policy (GaussianPolicyNetwork): Policy network.\n",
    "        optimizer (torch.optim.Optimizer): Optimizer for training.\n",
    "        device (str): Device to run the training on.\n",
    "        episodes (int, optional): Number of episodes to train for. Defaults to 1000.\n",
    "    \"\"\"\n",
    "    policy = policy.to(device)\n",
    "    \n",
    "    env = ContEnvWrapper()\n",
    "    gamma = 0.99\n",
    "    \n",
    "    for episode in range(episodes):\n",
    "        log_probs = []\n",
    "        rewards = []\n",
    "        env.reset()\n",
    "        state, _, _ = env.step(np.array([0,0,0]))\n",
    "        \n",
    "        state = torch.tensor(state).to(device)\n",
    "        \n",
    "        while True:\n",
    "            action, log_prob = select_action(policy, state)\n",
    "            next_state, reward, done = env.step([action.detach().cpu()])\n",
    "            \n",
    "            log_probs.append(log_prob)\n",
    "            rewards.append(reward)\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "            \n",
    "            state = torch.tensor(next_state).to(device)\n",
    "        \n",
    "        # Compute discounted rewards\n",
    "        R = 0\n",
    "        returns = []\n",
    "        for r in reversed(rewards):\n",
    "            R = r + gamma * R\n",
    "            returns.insert(0, R)\n",
    "        \n",
    "        returns = torch.tensor(returns).to(device)\n",
    "        returns = (returns - returns.mean()) / (returns.std() + 1e-7)  # Normalize\n",
    "        \n",
    "        # Update policy\n",
    "        policy_loss = []\n",
    "        for log_prob, R in zip(log_probs, returns):\n",
    "            policy_loss.append(-log_prob * R)\n",
    "        policy_loss = torch.cat([loss.view(1,-1) for loss in policy_loss]).sum()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        policy_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        print(f\"Episode {episode + 1}:\\t Mean Reward = {np.mean(rewards)} \\t Total Reward = {sum(rewards)} \\t Total Steps = {len(rewards)}\")\n",
    "\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize the input and output dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_28582/2751855686.py:67: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  state_tensor = torch.tensor(state, dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1:\t Mean Reward = 81.00816272177637 \t Total Reward = 40018.0323845575\n",
      "Episode 2:\t Mean Reward = 88.04243897207677 \t Total Reward = 52649.37850530193\n",
      "Episode 3:\t Mean Reward = -240.3504138454763 \t Total Reward = -95419.11429665406\n",
      "Episode 4:\t Mean Reward = 24.41920174928198 \t Total Reward = 5054.774762101375\n",
      "Episode 5:\t Mean Reward = 79.74944109981193 \t Total Reward = 34053.01134961967\n",
      "Episode 6:\t Mean Reward = 77.58061861002169 \t Total Reward = 19162.41279667536\n",
      "Episode 7:\t Mean Reward = 26.017684684833764 \t Total Reward = 10198.932396454882\n",
      "Episode 8:\t Mean Reward = 89.29094096666134 \t Total Reward = 20894.080186198757\n",
      "Episode 9:\t Mean Reward = 100.5287141734367 \t Total Reward = 36994.56681582472\n",
      "Episode 10:\t Mean Reward = 45.67844967604238 \t Total Reward = 12835.644358967897\n",
      "Episode 11:\t Mean Reward = 134.53806079865328 \t Total Reward = 65923.6497913401\n",
      "Episode 12:\t Mean Reward = 95.4437001634494 \t Total Reward = 29396.6596503424\n",
      "Episode 13:\t Mean Reward = 5.8659853143908 \t Total Reward = 774.310061499586\n",
      "Episode 14:\t Mean Reward = -30061.740195428418 \t Total Reward = -10491547.328204516\n",
      "Episode 15:\t Mean Reward = 122.12504084944831 \t Total Reward = 62039.52075151971\n",
      "Episode 16:\t Mean Reward = 32.834736679162354 \t Total Reward = 9423.569426919606\n",
      "Episode 17:\t Mean Reward = 113.60353875541807 \t Total Reward = 57710.5976877524\n",
      "Episode 18:\t Mean Reward = 150.50113315625242 \t Total Reward = 86989.65496431392\n",
      "Episode 19:\t Mean Reward = 60.14157326902055 \t Total Reward = 20748.84277781208\n",
      "Episode 20:\t Mean Reward = 100.28477281001031 \t Total Reward = 40414.7634424342\n",
      "Episode 21:\t Mean Reward = -136.64737627860913 \t Total Reward = -74746.11482439922\n",
      "Episode 22:\t Mean Reward = 128.7631705947023 \t Total Reward = 45968.45190230878\n",
      "Episode 23:\t Mean Reward = 74.14738454262174 \t Total Reward = 32105.817506955213\n",
      "Episode 24:\t Mean Reward = 69.32440527123742 \t Total Reward = 29601.521050818388\n",
      "Episode 25:\t Mean Reward = 140.93094752259 \t Total Reward = 64264.51207030109\n",
      "Episode 26:\t Mean Reward = 25.361511299335465 \t Total Reward = 6391.100847432535\n",
      "Episode 27:\t Mean Reward = 35.084388509523556 \t Total Reward = 18805.232241104644\n",
      "Episode 28:\t Mean Reward = 94.50776504120952 \t Total Reward = 29769.945987980984\n",
      "Episode 29:\t Mean Reward = 183.28276425938486 \t Total Reward = 94024.05806506438\n",
      "Episode 30:\t Mean Reward = 167.5826851420337 \t Total Reward = 101890.27256635657\n",
      "Episode 31:\t Mean Reward = 95.78112592892539 \t Total Reward = 37737.76361599656\n",
      "Episode 32:\t Mean Reward = 117.59087009247186 \t Total Reward = 62440.75201910255\n",
      "Episode 33:\t Mean Reward = -3170.8548319836264 \t Total Reward = -1487130.9162003202\n",
      "Episode 34:\t Mean Reward = 63.60837558163089 \t Total Reward = 24425.616223346275\n",
      "Episode 35:\t Mean Reward = 67.67205835418507 \t Total Reward = 31941.211543175308\n",
      "Episode 36:\t Mean Reward = -1492.3449947049614 \t Total Reward = -293991.96395687736\n",
      "Episode 37:\t Mean Reward = 136.05642563291104 \t Total Reward = 118369.09030063266\n",
      "Episode 38:\t Mean Reward = 158.8531305419496 \t Total Reward = 40984.10767982301\n",
      "Episode 39:\t Mean Reward = 117.9854167490033 \t Total Reward = 50969.70003556946\n",
      "Episode 40:\t Mean Reward = -2051.2172447361268 \t Total Reward = -443062.92486300337\n",
      "Episode 41:\t Mean Reward = 98.83486820590926 \t Total Reward = 43289.67227418826\n",
      "Episode 42:\t Mean Reward = 141.11058164003938 \t Total Reward = 71119.73314657985\n",
      "Episode 43:\t Mean Reward = 150.689722564864 \t Total Reward = 84687.62408145358\n",
      "Episode 44:\t Mean Reward = 101.81789998459061 \t Total Reward = 30036.280495454233\n",
      "Episode 45:\t Mean Reward = 6.4873948641438774 \t Total Reward = 2322.487361363508\n",
      "Episode 46:\t Mean Reward = 111.44554287452289 \t Total Reward = 66310.09801034114\n"
     ]
    }
   ],
   "source": [
    "# Define input and output dimensions\n",
    "input_dim = 578   # Pendulum state space\n",
    "output_dim = 3 # Pendulum action space\n",
    "\n",
    "# Initialize the policy network\n",
    "policy = TransformerGaussianPolicy(input_dim, output_dim, nhead=4, num_encoder_layers=4)\n",
    "\n",
    "# Set the device to CUDA if available, otherwise use CPU\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Initialize the optimizer with the policy network parameters and learning rate\n",
    "optimizer = optim.Adam(policy.parameters(), lr=1e-4)\n",
    "\n",
    "# Train the policy network for a specified number of episodes\n",
    "train_policy(policy, optimizer, device, episodes=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_agent():\n",
    "    \"\"\"\n",
    "    Function to test an agent using the environment.\n",
    "    \"\"\"\n",
    "    # Initialize the environment\n",
    "    config = Config()\n",
    "    env = ContEnvWrapper()\n",
    "    env.reset()\n",
    "\n",
    "    num_iterations = 1000\n",
    "    policy = policy.to('cpu')\n",
    "\n",
    "    obs, reward, done = env.step([0,0,0])\n",
    "    with torch.no_grad():\n",
    "        for i in range(num_iterations):\n",
    "            # Select an action based on the current policy\n",
    "            action, _ = select_action(policy, obs)\n",
    "\n",
    "            # Take a step in the environment\n",
    "            env.step(action)\n",
    "\n",
    "            # Render the environment\n",
    "            env.render()\n",
    "\n",
    "            # Pause for a short time to visualize the agent's actions\n",
    "            time.sleep(0.02)\n",
    "\n",
    "    # Close the environment\n",
    "    env.close()\n",
    "\n",
    "# Call the test_agent function to execute the code\n",
    "test_agent()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
