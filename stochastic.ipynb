{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agar.Env import AgarEnv\n",
    "import numpy as np\n",
    "render = True\n",
    "num_agents = 1\n",
    "import time\n",
    "\n",
    "class Args:\n",
    "    def __init__(self):\n",
    "\n",
    "        self.num_controlled_agent = num_agents\n",
    "        self.num_processes = 64\n",
    "        self.action_repeat = 1\n",
    "        self.total_step = 1e8\n",
    "        self.r_alpha = 0.1\n",
    "        self.r_beta = 0.1\n",
    "        self.seed = 42\n",
    "        self.gamma = 0.99\n",
    "        self.eval = True\n",
    "\n",
    "\n",
    "class ContEnvWrapper():\n",
    "    def __init__(self):\n",
    "        self.env = AgarEnv(Args())\n",
    "        self.action_limits = np.array([[-1,1],[-1,1]])\n",
    "        pass\n",
    "    \n",
    "    \n",
    "    def reset(self):\n",
    "        obs = self.env.reset()\n",
    "        return obs\n",
    "    \n",
    "    def render(self):\n",
    "        self.env.render(0,render_player=True)\n",
    "    \n",
    "    def close(self):\n",
    "        self.env.close()\n",
    "    def step(self, actions):\n",
    "        actions = np.array(actions).reshape(-1)\n",
    "        # actions = actions.reshape(-1)\n",
    "        actions[2] = 1 if actions[2] > 0 else 0\n",
    "        obs, rewards, dones, infos, new_obs = self.env.step(actions )\n",
    "        return obs['t0'], rewards[0], dones[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class TransformerGaussianPolicy(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, nhead, num_encoder_layers):\n",
    "        super(TransformerGaussianPolicy, self).__init__()\n",
    "        \n",
    "        # Embedding for the input, increase the dimension for transformer\n",
    "        self.embedding = nn.Linear(input_dim, 512)\n",
    "        \n",
    "        # Transformer encoder\n",
    "        self.transformer = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(d_model=512, nhead=nhead),\n",
    "            num_layers=num_encoder_layers\n",
    "        )\n",
    "        \n",
    "        # Heads for mean and standard deviation\n",
    "        self.mu_head = nn.Linear(512, output_dim)\n",
    "        self.sigma_head = nn.Linear(512, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        \n",
    "        # Note: Transformer expects input in the format (sequence length, batch size, features)\n",
    "        # Here, we treat our input as a sequence of length 1.\n",
    "        x = x.unsqueeze(0)\n",
    "        x = self.transformer(x)\n",
    "        x = x.squeeze(0)\n",
    "        \n",
    "        mu = torch.tanh(self.mu_head(x))\n",
    "        sigma = F.softplus(self.sigma_head(x)) + 1e-5\n",
    "        return mu, sigma\n",
    "\n",
    "\n",
    "class GaussianBoostedPolicyNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(GaussianPolicyNetwork, self).__init__()\n",
    "        self.fc = nn.Linear(input_dim, 128)\n",
    "        self.mu_head = nn.Linear(128, output_dim)\n",
    "        self.sigma_head = nn.Linear(128, output_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc(x))\n",
    "        mu = torch.tanh(self.mu_head(x))  # Mean\n",
    "        sigma = F.softplus(self.sigma_head(x)) + 1e-5  # Standard deviation\n",
    "        return mu, sigma\n",
    "    \n",
    "    \n",
    "    \n",
    "class GaussianPolicyNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(GaussianPolicyNetwork, self).__init__()\n",
    "        self.fc = nn.Linear(input_dim, 128)\n",
    "        self.mu_head = nn.Linear(128, output_dim)\n",
    "        self.sigma_head = nn.Linear(128, output_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc(x))\n",
    "        mu = torch.tanh(self.mu_head(x))  # Mean\n",
    "        sigma = F.softplus(self.sigma_head(x)) + 1e-5  # Standard deviation\n",
    "        return mu, sigma\n",
    "\n",
    "def select_action(policy, state):\n",
    "    state_tensor = torch.tensor(state, dtype=torch.float32)\n",
    "    mu, sigma = policy(state_tensor)\n",
    "    dist = torch.distributions.Normal(mu, sigma)\n",
    "    action = dist.sample()\n",
    "    log_prob = dist.log_prob(action).sum(-1)\n",
    "    return action, log_prob\n",
    "\n",
    "def train_policy(policy, optimizer, device, episodes=1000):\n",
    "    policy = policy.to(device)\n",
    "    \n",
    "    env = ContEnvWrapper()\n",
    "    gamma = 0.99\n",
    "    \n",
    "    for episode in range(episodes):\n",
    "        log_probs = []\n",
    "        rewards = []\n",
    "        env.reset()\n",
    "        state, _, _ = env.step(np.array([0,0,0]))\n",
    "        \n",
    "        state = torch.tensor(state).to(device)\n",
    "        \n",
    "        while True:\n",
    "            action, log_prob = select_action(policy, state)\n",
    "            next_state, reward, done = env.step([action.detach().cpu()])\n",
    "            \n",
    "            log_probs.append(log_prob)\n",
    "            rewards.append(reward)\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "            \n",
    "            state = torch.tensor(next_state).to(device)\n",
    "        \n",
    "        # Compute discounted rewards\n",
    "        R = 0\n",
    "        returns = []\n",
    "        for r in reversed(rewards):\n",
    "            R = r + gamma * R\n",
    "            returns.insert(0, R)\n",
    "        \n",
    "        returns = torch.tensor(returns).to(device)\n",
    "        returns = (returns - returns.mean()) / (returns.std() + 1e-7)  # Normalize\n",
    "        \n",
    "        # Update policy\n",
    "        policy_loss = []\n",
    "        for log_prob, R in zip(log_probs, returns):\n",
    "            policy_loss.append(-log_prob * R)\n",
    "        policy_loss = torch.cat([loss.view(1,-1) for loss in policy_loss]).sum()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        policy_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        print(f\"Episode {episode + 1}:\\t Mean Reward = {np.mean(rewards)} \\t Total Reward = {sum(rewards)}\")\n",
    "\n",
    "    env.close()\n",
    "\n",
    "# if __name__ == \"__main__\":\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_6508/2751855686.py:67: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  state_tensor = torch.tensor(state, dtype=torch.float32)\n"
     ]
    }
   ],
   "source": [
    "input_dim = 578   # Pendulum state space\n",
    "output_dim = 3 # Pendulum action space\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "policy = TransformerGaussianPolicy(input_dim, output_dim, nhead=4, num_encoder_layers=4)\n",
    "#policy = GaussianPolicyNetwork(input_dim, output_dim)\n",
    "optimizer = optim.Adam(policy.parameters(), lr=1e-3)\n",
    "train_policy(policy, optimizer, device, episodes=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing an agent\n",
    "env = ContEnvWrapper()\n",
    "env.reset()\n",
    "num_iterations = 1000\n",
    "policy = policy.to('cpu')\n",
    "obs, reward, done = env.step([0,0,0])\n",
    "with torch.no_grad():\n",
    "    for i in range(num_iterations):\n",
    "        action, _ = select_action(policy, obs)\n",
    "        env.step(action)\n",
    "        env.render()\n",
    "        time.sleep(0.02)\n",
    "        \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lakomchik/micromamba/envs/rl_agar_sim/lib/python3.9/site-packages/torch/nn/modules/transformer.py:282: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    }
   ],
   "source": [
    "model = TransformerGaussianPolicy(input_dim = 578, output_dim =3, nhead=8, num_encoder_layers=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_6508/2751855686.py:67: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  state_tensor = torch.tensor(state, dtype=torch.float32)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([ 0.1749,  0.3517, -0.0889]), tensor(-1.5814, grad_fn=<SumBackward1>))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "select_action(model, torch.randn(578))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
