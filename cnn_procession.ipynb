{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision\n",
    "from torchvision import transforms, utils\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import os\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from img2vec_pytorch import Img2Vec\n",
    "import timm\n",
    "from tqdm.notebook import tqdm\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgarIoImagesLoader(DataLoader):\n",
    "    def __init__(self, history_len =5):\n",
    "        self.images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_len = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNRegression(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(RNNRegression, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.rnn = nn.RNN(input_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out, _ = self.rnn(x)\n",
    "        out = self.fc(out[:, -1, :])  # Take the last time step's output\n",
    "        return out\n",
    "    \n",
    "class Embeddings(torch.nn.Module):\n",
    "    def __init__(self, num_encoder_features = 512):\n",
    "        super().__init__()\n",
    "        self.encoder = timm.create_model('mobilenetv3_small', num_encoder_features)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.encoder(x)\n",
    "    \n",
    "class HistoryEmbedding(torch.nn.Module):\n",
    "    def __init__(self, num_encoder_features = 512, history_len = 10):\n",
    "        super().__init__()\n",
    "        self.history_len = history_len\n",
    "        self.num_encoder_features = num_encoder_features\n",
    "        self.encoder = timm.create_model('resnet18', num_classes=num_encoder_features)\n",
    "        self.regressor = RNNRegression(num_encoder_features, self.history_len, 3)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = torch.stack([self.encoder(x[:,i,:,:,:]) for i in range(self.history_len)]).view(-1, self.history_len, self.num_encoder_features)\n",
    "        return self.regressor(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, data_dir,history_len, transform=None):\n",
    "        \n",
    "        self.num_images_per_folder = 1000\n",
    "        self.num_folders = 10\n",
    "        self.history_len = history_len\n",
    "        self.dataset_length = self.num_images_per_folder * self.num_folders\n",
    "        self.data_dir = data_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.dataset_length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_in_folder_idx = idx % self.num_images_per_folder\n",
    "        folder_idx = idx // self.num_images_per_folder\n",
    "        images = []\n",
    "        actions = []\n",
    "        \n",
    "        for i in range(img_in_folder_idx - self.history_len +1, img_in_folder_idx+1):\n",
    "            if(i < 0):\n",
    "                images.append(np.zeros((224,224,3), dtype=np.uint8))\n",
    "                actions.append(np.zeros((3), dtype=np.float32))\n",
    "            else:\n",
    "                img = Image.open(os.path.join(self.data_dir, f'episode_{folder_idx}','image', f'{i}.jpg'))\n",
    "                images.append(np.array(img))\n",
    "                actions.append(np.load(os.path.join(self.data_dir, f'episode_{folder_idx}','arr', f'{i}.npy'),allow_pickle=True)[0]['action'])\n",
    "            \n",
    "        if self.transform:\n",
    "            for i in range(len(images)):\n",
    "                images[i] = self.transform(images[i])\n",
    "                actions[i] = torch.from_numpy(actions[i])\n",
    "        images = torch.stack(images)\n",
    "        # In this example, we're just returning images without labels. \n",
    "        # You would typically also return labels in a real-world scenario.\n",
    "        actions = actions[-1].float()\n",
    "        return images, actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_sample(data):\n",
    "    fig, axs = plt.subplots(1, history_len, figsize=(20, 20))\n",
    "    for i in range(history_len):\n",
    "        axs[i].imshow(data[0][i].permute(1, 2, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Resize((224,224)),transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])\n",
    "dataset = CustomImageDataset('data', history_len=history_len, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = DataLoader(dataset, batch_size=16, shuffle=True, num_workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = HistoryEmbedding()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ded19f2c066e487d91ad41b4c24f0930",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/625 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "loss_function = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)  # You can use other optimizers like SGD if desired\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "moedl = model.to(device)\n",
    "# Number of epochs\n",
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()  # Set the model to training mode\n",
    "    total_loss = 0.0\n",
    "    \n",
    "    for batch_data, batch_labels in tqdm(data_loader):\n",
    "        # Transfer data and labels to GPU if one is available\n",
    "        \n",
    "        batch_data = batch_data.to(device)\n",
    "        batch_labels = batch_labels.to(device)\n",
    "\n",
    "\n",
    "        # 2. Training Loop\n",
    "        optimizer.zero_grad()  # Zero the gradients\n",
    "        \n",
    "        predictions = model(batch_data)  # Get predictions from the model\n",
    "        \n",
    "        loss = loss_function(predictions, batch_labels)  # Compute the loss\n",
    "        loss.backward()  # Backpropagate the loss\n",
    "        optimizer.step()  # Update the model parameters\n",
    "        \n",
    "        total_loss += loss.item()  # Accumulate loss for monitoring\n",
    "    \n",
    "    # Print the average loss for this epoch\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {avg_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving model\n",
    "torch.save(model.state_dict(), 'model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = HistoryEmbedding()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.load_state_dict(torch.load(\"model.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference_model(img_sequence, model):\n",
    "    # preprocessing input\n",
    "    transform = transforms.Compose([transforms.ToTensor(), transforms.Resize((224,224)),transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])\n",
    "    input = []\n",
    "    for img in img_sequence:\n",
    "        input.append(transform(img).float())\n",
    "    input = torch.stack(input).unsqueeze(0)\n",
    "    input = input.to(device)\n",
    "    # inference                               \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # print(img_sequence.shape)\n",
    "        output = model(input)\n",
    "        return output.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(800, 800, 3)\n",
      "[[-8.8330299e-02 -3.3349384e-02  3.1739473e-05]]\n",
      "(800, 800, 3)\n",
      "[[-8.8324279e-02 -3.3347148e-02  2.8431416e-05]]\n",
      "(800, 800, 3)\n",
      "[[-8.8324934e-02 -3.3347387e-02  2.8789043e-05]]\n",
      "(800, 800, 3)\n",
      "[[-8.833006e-02 -3.334928e-02  3.159046e-05]]\n",
      "(800, 800, 3)\n",
      "[[-8.8320017e-02 -3.3345584e-02  2.6091933e-05]]\n",
      "(800, 800, 3)\n",
      "[[-8.8328898e-02 -3.3348862e-02  3.0964613e-05]]\n",
      "(800, 800, 3)\n",
      "[[-8.8323623e-02 -3.3346925e-02  2.8073788e-05]]\n",
      "(800, 800, 3)\n",
      "[[-8.8323772e-02 -3.3346970e-02  2.8163195e-05]]\n",
      "(800, 800, 3)\n",
      "[[-8.8324517e-02 -3.3347238e-02  2.8565526e-05]]\n",
      "(800, 800, 3)\n",
      "[[-8.8317722e-02 -3.3344764e-02  2.4855137e-05]]\n",
      "(800, 800, 3)\n",
      "[[-8.8323146e-02 -3.3346731e-02  2.7805567e-05]]\n",
      "(800, 800, 3)\n",
      "[[-8.8318765e-02 -3.3345137e-02  2.5421381e-05]]\n",
      "(800, 800, 3)\n",
      "[[-8.8325232e-02 -3.3347521e-02  2.8952956e-05]]\n",
      "(800, 800, 3)\n",
      "[[-8.8323474e-02 -3.3346850e-02  2.7999282e-05]]\n",
      "(800, 800, 3)\n",
      "[[-8.8324100e-02 -3.3347089e-02  2.8327107e-05]]\n",
      "(800, 800, 3)\n",
      "[[-8.8326722e-02 -3.3348072e-02  2.9787421e-05]]\n",
      "(800, 800, 3)\n",
      "[[-8.8326424e-02 -3.3347953e-02  2.9623508e-05]]\n",
      "(800, 800, 3)\n",
      "[[-8.8324696e-02 -3.3347312e-02  2.8669834e-05]]\n",
      "(800, 800, 3)\n",
      "[[-8.8326573e-02 -3.3348013e-02  2.9698014e-05]]\n",
      "(800, 800, 3)\n",
      "[[-8.8326722e-02 -3.3348043e-02  2.9757619e-05]]\n",
      "(800, 800, 3)\n",
      "[[-8.8327855e-02 -3.3348475e-02  3.0383468e-05]]\n",
      "(800, 800, 3)\n",
      "[[-8.8329852e-02 -3.3349205e-02  3.1486154e-05]]\n",
      "(800, 800, 3)\n",
      "[[-8.8329673e-02 -3.3349145e-02  3.1396747e-05]]\n",
      "(800, 800, 3)\n",
      "[[-8.8328511e-02 -3.3348728e-02  3.0755997e-05]]\n",
      "(800, 800, 3)\n",
      "[[-8.8327140e-02 -3.3348221e-02  3.0010939e-05]]\n",
      "(800, 800, 3)\n",
      "[[-8.8326126e-02 -3.3347834e-02  2.9444695e-05]]\n",
      "(800, 800, 3)\n",
      "[[-8.8327050e-02 -3.3348177e-02  2.9951334e-05]]\n",
      "(800, 800, 3)\n",
      "[[-8.8327140e-02 -3.3348221e-02  3.0010939e-05]]\n",
      "(800, 800, 3)\n",
      "[[-8.8327140e-02 -3.3348221e-02  3.0010939e-05]]\n",
      "(800, 800, 3)\n",
      "[[-8.8325709e-02 -3.3347685e-02  2.9221177e-05]]\n",
      "(800, 800, 3)\n",
      "[[-8.8328570e-02 -3.3348728e-02  3.0770898e-05]]\n",
      "(800, 800, 3)\n",
      "[[-8.8328302e-02 -3.3348639e-02  3.0636787e-05]]\n",
      "(800, 800, 3)\n",
      "[[-8.8328779e-02 -3.3348817e-02  3.0905008e-05]]\n",
      "(800, 800, 3)\n",
      "[[-8.8328570e-02 -3.3348728e-02  3.0770898e-05]]\n",
      "(800, 800, 3)\n",
      "[[-8.8326842e-02 -3.3348102e-02  2.9847026e-05]]\n",
      "(800, 800, 3)\n",
      "[[-8.8327199e-02 -3.3348236e-02  3.0040741e-05]]\n",
      "(800, 800, 3)\n",
      "[[-8.8324457e-02 -3.3347238e-02  2.8535724e-05]]\n",
      "(800, 800, 3)\n",
      "[[-8.8325590e-02 -3.3347655e-02  2.9161572e-05]]\n",
      "(800, 800, 3)\n",
      "[[-8.8325590e-02 -3.3347655e-02  2.9161572e-05]]\n",
      "(800, 800, 3)\n",
      "[[-8.8326544e-02 -3.3347998e-02  2.9683113e-05]]\n",
      "(800, 800, 3)\n",
      "[[-8.8326097e-02 -3.3347834e-02  2.9429793e-05]]\n",
      "(800, 800, 3)\n",
      "[[-8.8326424e-02 -3.3347953e-02  2.9608607e-05]]\n",
      "(800, 800, 3)\n",
      "[[-8.8327259e-02 -3.3348251e-02  3.0055642e-05]]\n",
      "(800, 800, 3)\n",
      "[[-8.8326186e-02 -3.3347864e-02  2.9489398e-05]]\n",
      "(800, 800, 3)\n",
      "[[-8.8324755e-02 -3.3347342e-02  2.8699636e-05]]\n",
      "(800, 800, 3)\n",
      "[[-8.832717e-02 -3.334822e-02  3.002584e-05]]\n",
      "(800, 800, 3)\n",
      "[[-8.8329136e-02 -3.3348951e-02  3.1098723e-05]]\n",
      "(800, 800, 3)\n",
      "[[-8.8325799e-02 -3.3347715e-02  2.9265881e-05]]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_33731/2184060335.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_iterations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"rgb_array\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrender_player\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mimgs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Skoltech/term_5/RL/final_project/rl_final_project/agar/Env.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, playeridx, mode, name, render_player)\u001b[0m\n\u001b[1;32m    820\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mgeom\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgeoms_to_render\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    821\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mviewer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_onetime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgeom\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 822\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mviewer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreturn_rgb_array\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"rgb_array\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    823\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    824\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrender_border\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Skoltech/term_5/RL/final_project/rl_final_project/agar/rendering.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, return_rgb_array, name)\u001b[0m\n\u001b[1;32m    138\u001b[0m             \u001b[0marr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwidth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m             \u001b[0marr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhis_graph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    141\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwindow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0monetime_geoms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/micromamba/envs/rl_final_project/lib/python3.7/copy.py\u001b[0m in \u001b[0;36mdeepcopy\u001b[0;34m(x, memo, _nil)\u001b[0m\n\u001b[1;32m    159\u001b[0m             \u001b[0mcopier\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"__deepcopy__\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcopier\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 161\u001b[0;31m                 \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmemo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m                 \u001b[0mreductor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdispatch_table\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# testing an agent\n",
    "from agar.Config import Config\n",
    "from agar.Env import AgarEnv\n",
    "import time\n",
    "class Args:\n",
    "    def __init__(self):\n",
    "        self.num_controlled_agent = num_agents\n",
    "        self.num_processes = 64\n",
    "        self.action_repeat = 1\n",
    "        self.total_step = 1e8\n",
    "        self.r_alpha = 0.1\n",
    "        self.r_beta = 0.1\n",
    "        self.seed = 42\n",
    "        self.gamma = 0.99\n",
    "        self.eval = True\n",
    "render = True\n",
    "num_agents = 1\n",
    "\n",
    "config = Config()\n",
    "env = AgarEnv(Args())\n",
    "env.reset()\n",
    "num_iterations = 400\n",
    "env.step([0,0,0])\n",
    "imgs = [np.zeros([224,224,3]) for i in range(history_len)]\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in range(num_iterations):\n",
    "        image = env.render(0, mode=\"rgb_array\", render_player=False)\n",
    "        print(image.shape)\n",
    "        imgs.append(image.copy())\n",
    "        imgs.pop(0)\n",
    "        action = inference_model(imgs, model)\n",
    "        print(action)\n",
    "        action = action.reshape(-1)\n",
    "        action[2] = 1 if action[2] > 0.5 else 0\n",
    "        env.render(0, render_player=True)\n",
    "        observations, rewards, done, info, new_obs = env.step(action)\n",
    "        time.sleep(0.01)\n",
    "        \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
